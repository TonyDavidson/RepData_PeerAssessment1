lpha <- .05
mu <- 12
sd <- 4
n <- 100
z <- qnorm(alpha)
m0 <- mu + z * sd / sqrt(n)
m0 # 11.34206
alpha <- .05
mu <- 12
sd <- 4
n <- 100
z <- qnorm(alpha)
m0 <- mu + z * sd / sqrt(n)
m0 # 11.34206
pharm <- data.frame(baseline = c(140, 138, 150, 148, 135),
week2 = c(132, 135, 151, 146, 130))
t.test(pharm$baseline, pharm$week2, alternative = "two.sided", paired = T) # p-value = 0.08652
# problem 3.CI
n <- 9
mu <- 1100
sd <- 30
alpha <- .05
tstat <- qt(1 - alpha/2, n - 1)
mu + c(-1, 1)*tstat*sd / sqrt(n) # 1076.94 1123.06
# Problem 4. P-value
library(stats)
binom.test(x = 3, n = 4, p = .5, alt = "greater") # p-value = 0.3125
n1 <- 9
n2 <- 9
df <- n1 + n2 - 2
meanTreat <- -3
meanPlacebo <- 1
sdTreat <- 1.5
sdPlacebo <- 1.8
pooledVar <- (sdTreat^2 * n1 + sdPlacebo^2 * n2)/df
se.diff <- sqrt(pooledVar/n1 + pooledVar/n2)
tstat <- (meanTreat - meanPlacebo) / se.diff
tstat
pValue <- 2 * pt(tstat, df = df)
pValue # 0.0001852248
n <- 100
mu <- .01
sd <- .04
power.t.test(n, delta = mu, sd = sd, type = "one.sample", alt = "one.sided")$power # 0.7989855
m <- 10
alpha <- .05
alphaFwer <- alpha / m
alphaFwer
pharm <- data.frame(baseline = c(140, 138, 150, 148, 135),
week2 = c(132, 135, 151, 146, 130))
t.test(pharm$baseline, pharm$week2, alternative = "two.sided", paired = T) # p-value = 0.08652
data(mtcars)
fit <- lm(mpg ~ factor(cyl) + wt, mtcars)
summary(fit)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit <- lm(y ~ x)
lm.influence(fit)$hat
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit <- lm(y~x)
lm.influence(fit)$hat
dfbetas(fit)
fit3 <- lm(mpg ~ factor(cyl)*wt, mtcars)
summary(fit3)
lm(mpg ~ I(wt * 0.5) + factor(cyl), data = mtcars)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit <- lm(y ~ x)
lm.influence(fit)$hat
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit <- lm(y~x)
lm.influence(fit)$hat
dfbetas(fit)
library(MASS)
?shuttle
data(shuttle)
str(shuttle)
names(shuttle)
?glm
fit <- glm(use ~ wind, family='binomial', shuttle)
exp(fit$coeff)
fit <- glm(use ~ wind + as.factor(magn), family='binomial', shuttle)
exp(fit$coeff)
data(InsectSprays)
outp <- exp(coef(glm(count ~ as.factor(spray) - 1, family="poisson", InsectSprays)))
outp
outp[1]/outp[2]
?offset
log(10)
x <- -5:5
y <- c(5.12, 3.93, 2.67, 1.87, 0.52, 0.08, 0.93, 2.05, 2.54, 3.87, 4.97)
plot(y ~ x)
d1 <- c(0, 0 ,0, 0, 0, 0, 1 , 1, 1 , 1 , 1)
d2 <- c(1, 1 ,1, 1, 1, 1, 0 , 0, 0, 0 , 0)
# y = c + d1 * x + d2 * x
summary(lm(y ~ d1*x))
Reproducible Research: Peer Assessment 1
==========================================
Created by Xiaodan Zhang on July 18, 2014
### Basic settings
```{r}
echo = TRUE # Always make code visible
options(scipen = 1) # Turn off scientific notations for numbers
```
### Loading and processing the data
```{r}
unzip("activity.zip")
data <- read.csv("activity.csv", colClasses = c("integer", "Date", "factor"))
data$month <- as.numeric(format(data$date, "%m"))
noNA <- na.omit(data)
rownames(noNA) <- 1:nrow(noNA)
head(noNA)
dim(noNA)
library(ggplot2)
```
### What is mean total number of steps taken per day?
For this part of the assignment, you can ignore the missing values in the dataset.
* Make a histogram of the total number of steps taken each day
```{r}
ggplot(noNA, aes(date, steps)) + geom_bar(stat = "identity", colour = "steelblue", fill = "steelblue", width = 0.7) + facet_grid(. ~ month, scales = "free") + labs(title = "Histogram of Total Number of Steps Taken Each Day", x = "Date", y = "Total number of steps")
```
* Calculate and report the mean and median total number of steps taken per day
Mean total number of steps taken per day:
```{r}
totalSteps <- aggregate(noNA$steps, list(Date = noNA$date), FUN = "sum")$x
mean(totalSteps)
```
Median total number of steps taken per day:
```{r}
median(totalSteps)
```
### What is the average daily activity pattern?
* Make a time series plot (i.e. type = "l") of the 5-minute interval (x-axis) and the average number of steps taken, averaged across all days (y-axis)
```{r}
avgSteps <- aggregate(noNA$steps, list(interval = as.numeric(as.character(noNA$interval))), FUN = "mean")
names(avgSteps)[2] <- "meanOfSteps"
ggplot(avgSteps, aes(interval, meanOfSteps)) + geom_line(color = "steelblue", size = 0.8) + labs(title = "Time Series Plot of the 5-minute Interval", x = "5-minute intervals", y = "Average Number of Steps Taken")
```
* Which 5-minute interval, on average across all the days in the dataset, contains the maximum number of steps?
```{r}
avgSteps[avgSteps$meanOfSteps == max(avgSteps$meanOfSteps), ]
```
### Imputing missing values
* The total number of rows with NAs:
```{r}
sum(is.na(data))
```
* Devise a strategy for filling in all of the missing values in the dataset. The strategy does not need to be sophisticated. For example, you could use the mean/median for that day, or the mean for that 5-minute interval, etc.
My strategy is to use the mean for that 5-minute interval to fill each NA value in the steps column.
* Create a new dataset that is equal to the original dataset but with the missing data filled in.
```{r}
newData <- data
for (i in 1:nrow(newData)) {
if (is.na(newData$steps[i])) {
newData$steps[i] <- avgSteps[which(newData$interval[i] == avgSteps$interval), ]$meanOfSteps
}
}
head(newData)
sum(is.na(newData))
```
* Make a histogram of the total number of steps taken each day and Calculate and report the mean and median total number of steps taken per day.
```{r}
ggplot(newData, aes(date, steps)) + geom_bar(stat = "identity",
colour = "steelblue",
fill = "steelblue",
width = 0.7) + facet_grid(. ~ month, scales = "free") + labs(title = "Histogram of Total Number of Steps Taken Each Day (no missing data)", x = "Date", y = "Total number of steps")
```
* Do these values differ from the estimates from the first part of the assignment? What is the impact of imputing missing data on the estimates of the total daily number of steps?
Mean total number of steps taken per day:
```{r}
newTotalSteps <- aggregate(newData$steps,
list(Date = newData$date),
FUN = "sum")$x
newMean <- mean(newTotalSteps)
newMean
```
Median total number of steps taken per day:
```{r}
newMedian <- median(newTotalSteps)
newMedian
```
Compare them with the two before imputing missing data:
```{r}
oldMean <- mean(totalSteps)
oldMedian <- median(totalSteps)
newMean - oldMean
newMedian - oldMedian
```
So, after imputing the missing data, the new mean of total steps taken per day is the same as that of the old mean; the new median of total steps taken per day is greater than that of the old median.
### Are there differences in activity patterns between weekdays and weekends?
* Create a new factor variable in the dataset with two levels -- "weekday" and "weekend" indicating whether a given date is a weekday or weekend day.
```{r}
head(newData)
newData$weekdays <- factor(format(newData$date, "%A"))
levels(newData$weekdays)
levels(newData$weekdays) <- list(weekday = c("Monday", "Tuesday",
"Wednesday",
"Thursday", "Friday"),
weekend = c("Saturday", "Sunday"))
levels(newData$weekdays)
table(newData$weekdays)
```
* Make a panel plot containing a time series plot (i.e. type = "l") of the 5-minute interval (x-axis) and the average number of steps taken, averaged across all weekday days or weekend days (y-axis).
```{r}
avgSteps <- aggregate(newData$steps,
list(interval = as.numeric(as.character(newData$interval)),
weekdays = newData$weekdays),
FUN = "mean")
names(avgSteps)[3] <- "meanOfSteps"
library(lattice)
xyplot(avgSteps$meanOfSteps ~ avgSteps$interval | avgSteps$weekdays,
layout = c(1, 2), type = "l",
xlab = "Interval", ylab = "Number of steps")
```
xlab = "Interval", ylab = "Number of steps")
Reproducible Research: Peer Assessment 1
install.packages("ggplot2")
==========================================
```{r}
totalSteps <- aggregate(noNA$steps, list(Date = noNA$date), FUN = "sum")$x
Reproducible Research: Peer Assessment 1
==========================================
## Coursera - Reproducible Research Stream 005 - Peer Assessment 1
##
##Student:    Tony Davidson
##
### Basic settings
```{r}
echo = TRUE # Always make code visible
options(scipen = 1) # Turn off scientific notations for numbers
```
### Loading and processing the data
```{r}
unzip("activity.zip")
data <- read.csv("activity.csv", colClasses = c("integer", "Date", "factor"))
data$month <- as.numeric(format(data$date, "%m"))
noNA <- na.omit(data)
rownames(noNA) <- 1:nrow(noNA)
head(noNA)
dim(noNA)
library(ggplot2)
install.packages(c("Formula", "httr", "markdown", "mime", "RCurl", "swirl", "xlsx"))
Reproducible Research: Peer Assessment 1
==========================================
## Coursera - Reproducible Research Stream 005 - Peer Assessment 1
##
##Student:    Tony Davidson
##
### Basic settings
```{r}
echo = TRUE # Always make code visible
options(scipen = 1) # Turn off scientific notations for numbers
```
### Loading and processing the data
```{r}
unzip("activity.zip")
data <- read.csv("activity.csv", colClasses = c("integer", "Date", "factor"))
data$month <- as.numeric(format(data$date, "%m"))
noNA <- na.omit(data)
rownames(noNA) <- 1:nrow(noNA)
head(noNA)
dim(noNA)
library(ggplot2)
Reproducible Research: Peer Assessment 1
==========================================
## Coursera - Reproducible Research Stream 005 - Peer Assessment 1
##
##Student:    Tony Davidson
##
### Basic settings
```{r}
echo = TRUE # Always make code visible
options(scipen = 1) # Turn off scientific notations for numbers
```
### Loading and processing the data
```{r}
unzip("activity.zip")
data <- read.csv("activity.csv", colClasses = c("integer", "Date", "factor"))
data$month <- as.numeric(format(data$date, "%m"))
noNA <- na.omit(data)
rownames(noNA) <- 1:nrow(noNA)
head(noNA)
dim(noNA)
library(ggplot2)
library(ggplot2)
data
install.packages("dplyr")
Load the data.
unzip("activity.zip"
)
```
run_analysis
============
Last updated `r as.character(Sys.time())` using `r R.version$version.string`.
Student:    `Anthony Davidson`    Class:   `Getting and Cleaning Data-005`
Instructions for project
------------------------
> The purpose of this project is to demonstrate your ability to collect, work with, and clean a data set. The goal is to prepare tidy data that can be used for later analysis. You will be graded by your peers on a series of yes/no questions related to the project. You will be required to submit: 1) a tidy data set as described below, 2) a link to a Github repository with your script for performing the analysis, and 3) a code book that describes the variables, the data, and any transformations or work that you performed to clean up the data called CodeBook.md. You should also include a README.md in the repo with your scripts. This repo explains how all of the scripts work and how they are connected.
>
> One of the most exciting areas in all of data science right now is wearable computing - see for example this article . Companies like Fitbit, Nike, and Jawbone Up are racing to develop the most advanced algorithms to attract new users. The data linked to from the course website represent data collected from the accelerometers from the Samsung Galaxy S smartphone. A full description is available at the site where the data was obtained:
>
> http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones
>
> Here are the data for the project:
>
> https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip
>
> You should create one R script called run_analysis.R that does the following.
>
> 1. Merges the training and the test sets to create one data set.**DONE**
> 2. Extracts only the measurements on the mean and standard deviation for each measurement.**DONE**
> 3. Uses descriptive activity names to name the activities in the data set.**DONE**
> 4. Appropriately labels the data set with descriptive activity names.**DONE**
> 5. Creates a second, independent tidy data set with the average of each variable for each activity and each subject.**DONE**
>
> Good luck!
**The codebook is at the end of this document.**
Load/Get Libraries, set Working directory
-------------
Load packages.
```{r}
packages <- c("data.table", "reshape2")
sapply(packages, require, character.only=TRUE, quietly=TRUE)
```
Set path.
```{r}
path <- getwd()
path
```
Get the data
------------
Download the file. Put it in the `UCI HAR Dataset` folder. **This was initially done on 19-07-2014 at 6:02 pm (GMT+8:00). To save time I won't download again.**
```{r, eval=FALSE}
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip"
f <- "Dataset.zip"
if (!file.exists(path)) {dir.create(path)}
download.file(url, file.path(path, f))
```
Unzip the file. **This was already done on 19-07-2014. To save time I won't extract again.**
```{r, eval=FALSE}
executable <- file.path("C:", "Program Files (x86)", "7-Zip", "7z.exe")
parameters <- "x"
cmd <- paste(paste0("\"", executable, "\""), parameters, paste0("\"", file.path(path, f), "\""))
system(cmd)
```
The archive put the files in a folder named `UCI HAR Dataset`. Set this folder as the input path. List the files here.
```{r}
pathIn <- file.path(path, "UCI HAR Dataset")
list.files(pathIn, recursive=TRUE)
```
**See the `README.txt` file in `r path` for detailed information on the dataset.**
For the purposes of this project, the files in the `Inertial Signals` folders are not used.
Read the files
--------------
Read the subject files.
```{r}
dtSubjectTrain <- fread(file.path(pathIn, "train", "subject_train.txt"))
dtSubjectTest <- fread(file.path(pathIn, "test" , "subject_test.txt" ))
```
Read the activity files.
```{r}
dtActivityTrain <- fread(file.path(pathIn, "train", "Y_train.txt"))
dtActivityTest <- fread(file.path(pathIn, "test" , "Y_test.txt" ))
```
Read the data files. `fread` seems to be giving me some trouble reading files. Using a helper function, read the file with `read.table` instead, then convert the resulting data frame to a data table. Return the data table.
```{r fileToDataTable}
fileToDataTable <- function (f) {
df <- read.table(f)
dt <- data.table(df)
}
dtTrain <- fileToDataTable(file.path(pathIn, "train", "X_train.txt"))
dtTest <- fileToDataTable(file.path(pathIn, "test" , "X_test.txt" ))
```
Merge the training and the test sets
------------------------------------
Concatenate the data tables.
```{r}
dtSubject <- rbind(dtSubjectTrain, dtSubjectTest)
setnames(dtSubject, "V1", "subject")
dtActivity <- rbind(dtActivityTrain, dtActivityTest)
setnames(dtActivity, "V1", "activityNum")
dt <- rbind(dtTrain, dtTest)
```
Merge columns.
```{r}
dtSubject <- cbind(dtSubject, dtActivity)
dt <- cbind(dtSubject, dt)
```
Set key.
```{r}
setkey(dt, subject, activityNum)
```
Extract only the mean and standard deviation
--------------------------------------------
Read the `features.txt` file. This tells which variables in `dt` are measurements for the mean and standard deviation.
```{r}
dtFeatures <- fread(file.path(pathIn, "features.txt"))
setnames(dtFeatures, names(dtFeatures), c("featureNum", "featureName"))
```
Subset only measurements for the mean and standard deviation.
```{r}
dtFeatures <- dtFeatures[grepl("mean\\(\\)|std\\(\\)", featureName)]
```
Convert the column numbers to a vector of variable names matching columns in `dt`.
```{r}
dtFeatures$featureCode <- dtFeatures[, paste0("V", featureNum)]
head(dtFeatures)
dtFeatures$featureCode
```
Subset these variables using variable names.
```{r}
select <- c(key(dt), dtFeatures$featureCode)
dt <- dt[, select, with=FALSE]
```
Use descriptive activity names
------------------------------
Read `activity_labels.txt` file. This will be used to add descriptive names to the activities.
```{r}
dtActivityNames <- fread(file.path(pathIn, "activity_labels.txt"))
setnames(dtActivityNames, names(dtActivityNames), c("activityNum", "activityName"))
```
Label with descriptive activity names
-----------------------------------------------------------------
Merge activity labels.
```{r}
dt <- merge(dt, dtActivityNames, by="activityNum", all.x=TRUE)
```
Add `activityName` as a key.
```{r}
setkey(dt, subject, activityNum, activityName)
```
Melt the data table to reshape it from a short and wide format to a tall and narrow format.
```{r}
dt <- data.table(melt(dt, key(dt), variable.name="featureCode"))
```
Merge activity name.
```{r}
dt <- merge(dt, dtFeatures[, list(featureNum, featureCode, featureName)], by="featureCode", all.x=TRUE)
```
Create a new variable, `activity` that is equivalent to `activityName` as a factor class.
Create a new variable, `feature` that is equivalent to `featureName` as a factor class.
```{r}
dt$activity <- factor(dt$activityName)
dt$feature <- factor(dt$featureName)
```
Seperate features from `featureName` using the helper function `grepthis`.
```{r grepthis}
grepthis <- function (regex) {
grepl(regex, dt$feature)
}
## Features with 2 categories
n <- 2
y <- matrix(seq(1, n), nrow=n)
x <- matrix(c(grepthis("^t"), grepthis("^f")), ncol=nrow(y))
dt$featDomain <- factor(x %*% y, labels=c("Time", "Freq"))
x <- matrix(c(grepthis("Acc"), grepthis("Gyro")), ncol=nrow(y))
dt$featInstrument <- factor(x %*% y, labels=c("Accelerometer", "Gyroscope"))
x <- matrix(c(grepthis("BodyAcc"), grepthis("GravityAcc")), ncol=nrow(y))
dt$featAcceleration <- factor(x %*% y, labels=c(NA, "Body", "Gravity"))
x <- matrix(c(grepthis("mean()"), grepthis("std()")), ncol=nrow(y))
dt$featVariable <- factor(x %*% y, labels=c("Mean", "SD"))
## Features with 1 category
dt$featJerk <- factor(grepthis("Jerk"), labels=c(NA, "Jerk"))
dt$featMagnitude <- factor(grepthis("Mag"), labels=c(NA, "Magnitude"))
## Features with 3 categories
n <- 3
y <- matrix(seq(1, n), nrow=n)
x <- matrix(c(grepthis("-X"), grepthis("-Y"), grepthis("-Z")), ncol=nrow(y))
dt$featAxis <- factor(x %*% y, labels=c(NA, "X", "Y", "Z"))
```
Check whether all possible combinations of `feature` are accounted for by all possible combinations of the factor class variables.
```{r}
r1 <- nrow(dt[, .N, by=c("feature")])
r2 <- nrow(dt[, .N, by=c("featDomain", "featAcceleration", "featInstrument", "featJerk", "featMagnitude", "featVariable", "featAxis")])
r1 == r2
```
Create a tidy data set
----------------------
Create a data set with the average of each variable for each activity and each subject.
```{r}
setkey(dt, subject, activity, featDomain, featAcceleration, featInstrument, featJerk, featMagnitude, featVariable, featAxis)
dtTidy <- dt[, list(count = .N, average = mean(value)), by=key(dt)]
```
Make codebook.
```{r}
knit("makeCodebook.Rmd", output="codebook.md", encoding="ISO8859-1", quiet=TRUE)
markdownToHTML("codebook.md", "codebook.html")
```
Reproducible Research: Peer Assessment 1
==========================================
## Coursera - Reproducible Research Stream 005
install.packages("miktex")
sreps
Reproducible Research: Peer Assessment 1
knit(PA1_template.Rmd,PA1_template.md)
setwd("~/GitHub/RepData_PeerAssessment1")
library(knitr)
knitr(PA1_template.Rmd,PA1_template.md)
knit(PA1_template.Rmd,PA1_template.md)
setwd("~/GitHub/RepData_PeerAssessment1")
library(knitr)
library(pandoc)
knit('PA1_template.Rmd')
pandoc('PA1_template.md')
library(knitr)
library(markdown)
knit("PA1_template.Rmd")
markdownToHTML("PA1_template.md", "PA1_template.html")
markdownToHTML("PA1_template.md", "PA1_template.html")
markdownToHTML("PA1_template.md", "PA1_template.html")
